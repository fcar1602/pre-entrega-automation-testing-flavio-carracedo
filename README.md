# Pre-Entrega: Selenium + Python Automation Testing

This repository contains a Selenium-based UI test suite implemented with Python and Pytest. It's organized as a simple page-object driven test project against the SauceDemo demo site (https://www.saucedemo.com). The intent is to provide a clean, reproducible pre-delivery (pre-entrega) automation testing submission.

## Repository name
pre-entrega-automation-testing-Flavio-Carracedo

---

## Table of contents
- Project status
- Prerequisites
- Installation
- How to run tests
- Recommended browser drivers
- Project structure
- Key design decisions
- Adding new tests
- CI / GitHub Actions guidance
- Troubleshooting
- Contributing / Commit message guidance

---

## Project status
- Tests currently exercise the SauceDemo application: login, navigation, add-to-cart flows.
- HTML report (`report.html`) is generated by pytest-html when tests are executed with appropriate addopts.

## Prerequisites
- Python 3.10+ (project was tested on Python 3.12)
- pip
- Git
- A browser (Chrome / Firefox) and the matching WebDriver (chromedriver/geckodriver)

## Installation
1. Create and activate a virtual environment (PowerShell):

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

2. Install dependencies 

```powershell
pip install -r requirements.txt

pip install selenium pytest pytest-html
```

## How to run tests with Report


```powershell
python -m pytest preEntrega/tests -v --html=report.html --self-contained-html
```

Run a single test file:

```powershell
python -m pytest preEntrega/tests/test_login.py -v --html=report.html --self-contained-html

```

## Recommended browser drivers
- Chrome: download `chromedriver` matching your Chrome version and place it in your PATH or in `drivers/` folder. 
- Firefox: download `geckodriver` and add to PATH.

For local debugging it's easiest to place the driver executable in a `drivers/` directory and update your `conftest.py` to point to it.

## Project structure
```
pre-entrega-automation-testing-[Flavio-Carracedo]
├── preEntrega/
│   ├── pages/                # Page objects (base_page, login_page, inventory_page, cart_page)
│   ├── tests/                # Pytest tests
│   ├── utils/                # Helpers and utility functions 
│   └── conftest.py           # Pytest fixtures (driver setup/teardown)
├── reports/screenshots       # Generated reports and screenshots
├── pytest.ini                # Pytest configuration
├── report.html               # Last generated html report
└── README.md
```

## Key design decisions
- Page Object Model: pages contain page-specific logic and locators. Reusable waiting and low-level helpers are in `utils/helpers.py`.
- Tests are written with pytest; fixtures are used to set up and tear down WebDriver instances.
- Logging in tests uses standard `logging` and `caplog` for capturing logs in html reports.

## Adding new tests
- Create or reuse page objects in `preEntrega/pages/`.
- Add higher-level actions to page classes, keep waits and low-level helpers in `preEntrega/utils/helpers.py`.
- Write tests in `preEntrega/tests/` using pytest and assert plain English messages.

## API Testing

API tests live under `preEntrega/tests_api` and validate authentication, profile retrieval, and a basic user lifecycle against DummyJSON.

- Location: `preEntrega/tests_api/`
  - `test_login_api.py`: Login (`POST /auth/login`) and fetch authenticated profile (`GET /auth/me`).
  - `test_users_api.py`: Login and verify `/user/me` returns expected user details.
  - `test_create_user_api.py`: Create user, update `lastName`, then delete the user.

### How to run only API tests

```powershell
python -m pytest -s preEntrega/tests_api -v
```

### HTML report for API tests

The project is configured (via `pytest.ini`) to generate a self-contained HTML report at `report.html`.

```powershell
python -m pytest -s preEntrega/tests_api -v --html=report.html --self-contained-html
```

### Separate HTML reports for E2E and API

You can generate separate reports using dedicated config files:

- E2E config: `pytest-e2e.ini` → writes `report.html`
- API config: `pytest-api.ini` → writes `reportApi.html`

Run commands:

```powershell
# E2E (UI) tests with screenshots
python -m pytest -c pytest-e2e.ini -s -v

# API tests with separate report
python -m pytest -c pytest-api.ini -s -v
```

### Fixtures and client

- `api` fixture: provides an `APIClient` initialized with base URL `https://dummyjson.com`.
- `APIClient.login(username, password)`: stores `accessToken` and sets `Authorization: Bearer <token>` header automatically.
- `fake` fixture: uses `Faker` to generate realistic payloads.
- Tests use `caplog` to emit structured logs that appear in the HTML report.

### Endpoints used

- Auth:
  - `POST /auth/login` → returns `accessToken`, `refreshToken`, and user details.
  - `GET /auth/me` → returns the authenticated user.
- Users:
  - `POST /users/add` → creates a user (mock response shape).
  - `PUT /users/{id}` → updates user fields (e.g., `lastName`).
  - `DELETE /users/{id}` → deletes a user.

Note: DummyJSON is a mock API. Some operations may not persist newly created IDs across update/delete, sometimes returning `404`. The tests log the chosen `id` and responses for clarity. If needed, use a known ID (e.g., `5`) for update/delete to demonstrate the flow.

### Quick commands

Run the login + profile test:

```powershell
python -m pytest -s preEntrega/tests_api/test_login_api.py -v
```

Run the full user lifecycle test with report:

```powershell
python -m pytest -s preEntrega/tests_api/test_create_user_api.py -v --html=report.html --self-contained-html
```

## CI / GitHub Actions guidance
- Create `.github/workflows/ci.yml` to run `pytest --html=report.html` in the matrix for desired browsers.
- Store `report.html` as a workflow artifact for later download.
- Use official actions to install matching browser and driver versions (for example, use `actions/setup-python` and `browser-actions/setup-chromedriver` or download driver manually).

## Troubleshooting
- Fixture 'driver' not found: ensure `conftest.py` is in `preEntrega/` or `preEntrega/tests/` and defines the `driver` fixture.
- Element timing issues: use helpers' `wait_for_visibility` / `wait_for_clickable`.
- Mismatched driver versions: update driver to match your browser.

## Contributing / Commit message guidance
- Make frequent, small commits with descriptive messages like:
  - `feat: add LoginPage and login test`
  - `fix: update selector for add-to-cart button`
  - `chore: add README and requirements`

## Contact
If you need help or want me to finalize the repository (create `requirements.txt`, add CI workflow, or run tests here), tell me which task to do next.
